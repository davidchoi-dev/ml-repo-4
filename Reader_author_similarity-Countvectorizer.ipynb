{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작업 내용\n",
    "- 선정기간 동안 5개 이상의 article을 읽은 독자에 한해서 분석 진행\n",
    "- 독자 1명이 읽은 글의 전체 태그를 말뭉치로 만듦\n",
    "- 말뭉치를 형태소로 분석\n",
    "    - Doc2vec 적용하여 유사한 독자 서칭\n",
    "    - 독자 간 cosine 유사도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  \n",
      "C:\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pandas import Panel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import Panel\n",
    "tqdm.pandas()\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Brunch_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json('C:/Users/eunice/dss_workspace/MLproject/data/metadata.json', lines=True)\n",
    "read_file_lst = glob.glob('C:/Users/eunice/dss_workspace/MLproject/data/read\\*')\n",
    "maga = pd.read_json('C:/Users/eunice/dss_workspace/MLproject/data/magazine.json', lines=True)\n",
    "us = pd.read_json('C:/Users/eunice/dss_workspace/MLproject/data/users.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Data(meta, read_file_lst, maga, us)\n",
    "me_df, r_df, mag_df, us_df = a.data_preprocessing()\n",
    "# 2/15 - 3/31 train 데이터\n",
    "train = a.train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_id, keyword_list\n",
    "key = a.train_keyword_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readers별 article list와 articlle count가 있는 df :ra\n",
    "ra_list_df = Read_article_outline_remove(train)\n",
    "ra = ra_list_df.read_article_list()\n",
    "# up = ra_list_df.upper_fence_remove()\n",
    "# 5 이상 upper fence 밑으로 정리\n",
    "# t3 = article_count_division_3(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_fence_df = ra[(ra['article_id_count']>=9) & (ra['article_id_count']<=64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_fence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 독자-독자 유사도 구하기\n",
    "- 독자별 주로 읽은 글의 키워드 정리하기\n",
    "- 1. train2 데이터와 read데이터의 독자 정보 merge\n",
    "- 2. train2 데이터에서 독자 id기준 groupby/ keyword list sum\n",
    "- 3. 키워드 중복 제거\n",
    "- 4. 키워드 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_fence_df.drop(['article_list', 'article_id_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. train 데이터와 upper_fence_df데이터의 독자 정보 merge (설정 기간동안 작성된 article에 대해서만 병합)\n",
    "train2 = train.merge(upper_fence_df, how='inner', on='readers_id')\n",
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 독자 아이디별 태그리스트 합치기\n",
    "train_key_sum = train2.groupby('readers_id')['keyword_list'].agg(sum)\n",
    "train_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum = train_key_sum.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_key_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드가 있는 작가만 남기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. 키워드가 있는 아이디만 남기기\n",
    "train_key_sum = train_key_sum[train_key_sum['keyword_list'].apply(lambda x: len(x))!=0]\n",
    "train_key_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드 중복 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 중복제거 명령\n",
    "tqdm.pandas()\n",
    "train_key_sum['keyword_list'] = train_key_sum['keyword_list'].progress_apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphs_analysis(data):\n",
    "    ls = []\n",
    "    for x in range(len(data)):\n",
    "        ls.append(kkma.morphs(data[x]))\n",
    "    return list(chain.from_iterable(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_key_sum['morphs_list'] = train_key_sum['keyword_list'].progress_apply(lambda x: morphs_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복제거 전 형태소분석 결과 저장하기\n",
    "# train_key_sum.to_csv('keyword_list_mporphs_duplicates.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 완료 (중복제거 버전)\n",
    "train_key_sum.to_csv('kewordlist_morphs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer 구성: 형태소 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *파일로 읽어올 경우! 꼭 replace가 들어간 apply를 실행해주세요* -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum = pd.read_csv('kewordlist_morphs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum['morphs_list'] = train_key_sum['morphs_list'].apply(lambda x: str(x).replace('[', '').replace(']','').replace(\"'\", ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countvectorizer instanciation, fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12022/12022 [00:00<00:00, 47485.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12022, 536260)\n"
     ]
    }
   ],
   "source": [
    "# keyword_list 유사도 측정 (countvectorizer을 이용해서 빈도수로 벡터화)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer를 적용하기 위해 공백문자로 word 단위가 구분되는 문자열로 변환\n",
    "tqdm.pandas()\n",
    "train_key_sum[\"keyword_list_literal\"] = train_key_sum['morphs_list'].progress_apply(lambda x: (\"\").join(x))\n",
    "train_key_sum[\"keyword_list_literal\"].dropna(inplace=True)\n",
    "# 이를 적용해 개별 요소를 공백 문자로 구분하는 문자열로 변환해 별도의 칼럼인 keyword_list_literal 칼럼으로 저장\n",
    "\n",
    "count_vect = CountVectorizer(min_df=0, ngram_range=(1,3)) \n",
    "# min_df : 최소 빈도수(최소 노출 빈도수 정해주는 것), ngram_range: 토큰화 단어갯수\n",
    "reader_keyword_mat = count_vect.fit_transform(train_key_sum[\"keyword_list_literal\"])\n",
    "print(reader_keyword_mat.shape)\n",
    "# 643104 레코드와 1088182개의 개별 단어 피처로 구성된 피처 벡터 행렬이 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 268392)\t1\n",
      "  (0, 24497)\t1\n",
      "  (0, 253604)\t1\n",
      "  (0, 343155)\t1\n",
      "  (0, 268401)\t1\n",
      "  (0, 26218)\t1\n",
      "  (0, 254102)\t1\n",
      "  (0, 268402)\t1\n",
      "  (0, 26221)\t1\n",
      "  (1, 465682)\t1\n",
      "  (1, 456095)\t1\n",
      "  (1, 161704)\t1\n",
      "  (1, 347161)\t1\n",
      "  (1, 285561)\t1\n",
      "  (1, 17590)\t1\n",
      "  (1, 336754)\t1\n",
      "  (1, 448138)\t1\n",
      "  (1, 288328)\t1\n",
      "  (1, 259879)\t1\n",
      "  (1, 244493)\t1\n",
      "  (1, 184808)\t1\n",
      "  (1, 382908)\t1\n",
      "  (1, 322304)\t1\n",
      "  (1, 374256)\t1\n",
      "  (1, 295584)\t1\n",
      "  :\t:\n",
      "  (12021, 399358)\t1\n",
      "  (12021, 425058)\t1\n",
      "  (12021, 290880)\t1\n",
      "  (12021, 471200)\t1\n",
      "  (12021, 227383)\t1\n",
      "  (12021, 471201)\t1\n",
      "  (12021, 330276)\t1\n",
      "  (12021, 254087)\t1\n",
      "  (12021, 193649)\t1\n",
      "  (12021, 473917)\t1\n",
      "  (12021, 19067)\t1\n",
      "  (12021, 515580)\t1\n",
      "  (12021, 303742)\t1\n",
      "  (12021, 515582)\t1\n",
      "  (12021, 397052)\t1\n",
      "  (12021, 265859)\t1\n",
      "  (12021, 473918)\t1\n",
      "  (12021, 368576)\t1\n",
      "  (12021, 290884)\t1\n",
      "  (12021, 398175)\t1\n",
      "  (12021, 330280)\t1\n",
      "  (12021, 274572)\t1\n",
      "  (12021, 405179)\t1\n",
      "  (12021, 95834)\t1\n",
      "  (12021, 344988)\t1\n"
     ]
    }
   ],
   "source": [
    "print(reader_keyword_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector 간 cosine 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.0 GiB for an array with shape (12022, 536260) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2c6e9b89dabb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcos_reader_keyword_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader_keyword_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader_keyword_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_reader_keyword_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_reader_keyword_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.0 GiB for an array with shape (12022, 536260) and data type int64"
     ]
    }
   ],
   "source": [
    "cos_reader_keyword_mat = cosine_similarity(reader_keyword_mat.toarray(), reader_keyword_mat.toarray())\n",
    "print(cos_reader_keyword_mat.shape)\n",
    "print(cos_reader_keyword_mat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cos_reader_keyword_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_keyword_sim_sorted_ind = cos_reader_keyword_mat.argsort()[:,::-1]\n",
    "print(reader_keyword_sim_sorted_ind[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_article(df, sorted_ind, readers_name, top_n=10):\n",
    "    reader_list = df[df['readers_id']==readers_name]\n",
    "    \n",
    "    article_index = reader_list.index.values\n",
    "    similar_indexes = sorted_ind[article_index, :(top_n)]\n",
    "    \n",
    "    print(similar_indexes)\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "    \n",
    "\n",
    "    return df.iloc[similar_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key_sum[train_key_sum['readers_id']=='#fffe67ecc0056dd26ae00511957c5a2b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine 유사도 기준 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "similar_article = find_sim_article(train_key_sum, reader_keyword_sim_sorted_ind, '#fffe67ecc0056dd26ae00511957c5a2b', 10)\n",
    "similar_article[['readers_id', 'keyword_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작가-작가 유사도 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 작가별 키워드 말뭉치 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 read하기\n",
    "author_key_sum = train.groupby('author_id')['keyword_list'].agg('sum')\n",
    "author_key_sum = author_key_sum.reset_index()\n",
    "author_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(author_key_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 작가별 주로 사용한 글의 키워드\n",
    "author_key_sum = author_key_sum[author_key_sum['keyword_list'].apply(lambda x: len(x))!=0]\n",
    "author_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 중복제거 명령\n",
    "tqdm.pandas()\n",
    "author_key_sum['k_list'] = author_key_sum['keyword_list'].progress_apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_key_sum.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_key_sum['morphs_list'] = author_key_sum['k_list'].progress_apply(lambda x: morphs_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 결과 저장\n",
    "author_key_sum.to_csv('author_keyword_list_unique.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer 구성: 형태소 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_key_sum['morphs_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countvectorizer instanciation, fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_list 유사도 측정 (countvectorizer을 이용해서 빈도수로 벡터화)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer를 적용하기 위해 공백문자로 word 단위가 구분되는 문자열로 변환\n",
    "author_key_sum[\"keyword_list_literal\"] = author_key_sum['morphs_list'].progress_apply(lambda x: (\" \").join(x))\n",
    "author_key_sum[\"keyword_list_literal\"].dropna(inplace=True)\n",
    "# 이를 적용해 개별 요소를 공백 문자로 구분하는 문자열로 변환해 별도의 칼럼인 keyword_list_literal 칼럼으로 저장\n",
    "\n",
    "count_vect = CountVectorizer(min_df=0, ngram_range=(1,3)) \n",
    "# min_df : 최소 빈도수(최소 노출 빈도수 정해주는 것), ngram_range: 토큰화 단어갯수\n",
    "author_keyword_mat = count_vect.fit_transform(author_key_sum[\"keyword_list_literal\"])\n",
    "print(author_keyword_mat.shape)\n",
    "# 643104 레코드와 1088182개의 개별 단어 피처로 구성된 피처 벡터 행렬이 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(author_keyword_mat.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector 간 cosine 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_author_keyword_mat = cosine_similarity(author_keyword_mat.toarray(), author_keyword_mat.toarray())\n",
    "print(cos_author_keyword_mat.shape)\n",
    "print(cos_author_keyword_mat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cos_author_keyword_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_keyword_sim_sorted_ind = cos_author_keyword_mat.argsort()[:,::-1]\n",
    "print(author_keyword_sim_sorted_ind[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_author(df, sorted_ind, author_name, top_n=10):\n",
    "    author_list = df[df['author_id']==author_name]\n",
    "    \n",
    "    article_index = author_list.index.values\n",
    "    similar_indexes = sorted_ind[article_index, :(top_n)]\n",
    "    \n",
    "    print(similar_indexes)\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "\n",
    "    return df.iloc[similar_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_key_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdata에만 있음\n",
    "author_key_sum[author_key_sum['author_id']== '@13july']['keyword_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine 유사도 기준 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_similar_article = find_sim_author(author_key_sum, author_keyword_sim_sorted_ind, '@13july', 50)\n",
    "author_similar_article[['author_id','keyword_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 독자-작가 간 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(author_keyword_mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_keyword_mat[0].to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(reader_keyword_mat[0], author_keyword_mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_author_keyword_mat = cosine_similarity(author_keyword_mat, reader_keyword_mat)\n",
    "print(reader_author_keyword_mat.shape)\n",
    "print(reader_author_keyword_mat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reader_author_keyword_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_author_keyword_sim_sorted_ind = reader_author_keyword_mat.argsort()[:,::-1]\n",
    "print(reader_author_keyword_mat_sorted_ind[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_reader_author(df, sorted_ind, readers_name, top_n=10):\n",
    "    reader_list = df[df['author_id']==readers_name]\n",
    "    \n",
    "    article_index = reader_list.index.values\n",
    "    similar_indexes = sorted_ind[article_index, :(top_n)]\n",
    "    \n",
    "    print(similar_indexes)\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "    \n",
    "\n",
    "    return df.iloc[similar_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdata에만 있음\n",
    "author_keyword_sum[author_keyword_sum['author_id']== '@13july']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_sim[27964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine 유사도 기준 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_similar_article = find_sim_author(author_keyword_sum, author_keyword_sim_sorted_ind, '#d39d7f99b55209e6ecd651481bda9a66', 10)\n",
    "author_similar_article[['author_id', 'keyword_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
